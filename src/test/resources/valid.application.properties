# What topics are searched from kafka, regex
queueTopicPattern=^testConsumerTopic-*$
# Number of consumers created to the consumer groups
numOfConsumers=2
# Kafka bootstrap servers
consumer.bootstrap.servers=test
# Offset, should not be touched
consumer.auto.offset.reset=earliest
# Autocommit, should not be touched
consumer.enable.auto.commit=false
# Consumer group id, this is to track the progress of reading hte topic
consumer.group.id=cfe_39
# Used security protocol and mechanism
consumer.security.protocol=SASL_PLAINTEXT
consumer.sasl.mechanism=PLAIN
# Maximum records per batch, note that too big number will cause massive load and can cause timeouts to trigger
consumer.max.poll.records=500
# How much data can be fetched in one go
consumer.fetch.max.bytes=1073741820
# How long for request before timing out. Note that too big max poll records size can cause this to trigger
consumer.request.timeout.ms=300000
consumer.max.poll.interval.ms=300000
# For testing only, remove for prod.
consumer.useMockKafkaConsumer=true
# The maximum file size for AVRO-files that are to be stored in HDFS database.
maximumFileSize=3000
# Boolean for deciding if records not in RFC5424 should be skipped or not.
skipNonRFC5424Records=true
# Boolean for deciding if empty RFC5424 records should be skipped or not.
skipEmptyRFC5424Records=true
# HDFS pruning, use 157784760000 value while testing HDFS writes to ensure the test records are not pruned. 157784760000L
pruneOffset=157784760000
# HDFS uri
hdfsuri=hdfs://localhost:45937/
# Kerberos
java.security.krb5.kdc=test
java.security.krb5.realm=test
hadoop.security.authentication=test
hadoop.security.authorization=test
dfs.namenode.kerberos.principal.pattern=test
KerberosKeytabUser=test
KerberosKeytabPath=test
dfs.client.use.datanode.hostname=false
kerberosLoginAutorenewal=true
dfs.data.transfer.protection=test
dfs.encrypt.data.transfer.cipher.suites=test