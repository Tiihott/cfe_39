# Kafka security configuration file
java.security.auth.login.config=/opt/teragrep/cfe_39/etc/config.jaas
# Logger settings
log4j2.configurationFile=/opt/teragrep/cfe_39/etc/log4j2.properties
# What topics are searched from kafka, regex
queueTopicPattern=^testConsumerTopic-*$
# Number of consumers created to the consumer groups
numOfConsumers=2
# Kafka bootstrap servers
consumer.bootstrap.servers=test
# Offset, should not be touched
consumer.auto.offset.reset=earliest
# Autocommit, should not be touched
consumer.enable.auto.commit=false
# Consumer group id, this is to track the progress of reading hte topic
consumer.group.id=cfe_39
# Used security protocol and mechanism
consumer.security.protocol=SASL_PLAINTEXT
consumer.sasl.mechanism=PLAIN
# Maximum records per batch, note that too big number will cause massive load and can cause timeouts to trigger
consumer.max.poll.records=500
# How much data can be fetched in one go
consumer.fetch.max.bytes=1073741820
# How long for request before timing out. Note that too big max poll records size can cause this to trigger
consumer.request.timeout.ms=300000
consumer.max.poll.interval.ms=300000
# For testing only, remove for prod.
consumer.useMockKafkaConsumer=true
# Directory where AVRO files are constructed for HDFS
queueDirectory=/opt/teragrep/cfe_39/etc/AVRO/
# The maximum file size for AVRO-files that are to be stored in HDFS database.
maximumFileSize=60800000
# Boolean for deciding if records not in RFC5424 should be skipped or not.
skipNonRFC5424Records=true
# Boolean for deciding if empty RFC5424 records should be skipped or not.
skipEmptyRFC5424Records=true
# HDFS pruning offset, prunes files older than the given milliseconds.
pruneOffset=172800000
# HDFS uri
hdfsuri=hdfs://localhost:45937/
# Kerberos
java.security.krb5.kdc=test
java.security.krb5.realm=test
hadoop.security.authentication=test
hadoop.security.authorization=test
dfs.namenode.kerberos.principal.pattern=test
KerberosKeytabUser=test
KerberosKeytabPath=test
dfs.client.use.datanode.hostname=false